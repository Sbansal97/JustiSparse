{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sumita/miniconda3/envs/justisparse/lib/python3.9/site-packages/ekphrasis/classes/tokenizer.py:225: FutureWarning: Possible nested set at position 2190\n",
      "  self.tok = re.compile(r\"({})\".format(\"|\".join(pipeline)))\n",
      "/home/sumita/miniconda3/envs/justisparse/lib/python3.9/site-packages/ekphrasis/classes/exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
      "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
     ]
    }
   ],
   "source": [
    "text_processor = TextPreProcessor(\n",
    "    # terms that will be normalized\n",
    "    normalize=['email', 'user', 'url'],\n",
    "    # normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
    "    #     'time', 'url', 'date', 'number'],\n",
    "    # terms that will be annotated\n",
    "    annotate={\"elongated\", \"repeated\",\n",
    "        'emphasis', 'censored'},\n",
    "    fix_html=False,  # fix HTML tokens\n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for word segmentation \n",
    "    segmenter=\"twitter\", \n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for spell correction\n",
    "    corrector=\"twitter\", \n",
    "    \n",
    "    unpack_hashtags=False,  # perform word segmentation on hashtags\n",
    "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
    "    spell_correct_elong=False,  # spell correction for elongated words\n",
    "    \n",
    "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
    "    # the tokenizer, should take as input a string and return a list of tokens\n",
    "    tokenizer=SocialTokenizer(lowercase=False).tokenize,\n",
    "    \n",
    "    # list of dictionaries, for replacing tokens extracted from the text,\n",
    "    # with other expressions. You can pass more than one dictionaries.\n",
    "    dicts=[emoticons]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_processor = lambda x: ' '.join(text_processor.pre_process_doc(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2177316/2871980637.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_data['tweet'] = filtered_data['tweet'].map(tweet_processor)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('../data/fdcl/toxic_lang_data_pub/ND_founta_trn_dial_pAPI.csv')\n",
    "\n",
    "filtered_data = data[['id','tweet','dialect_argmax','ND_label']]\n",
    "filtered_data['tweet'] = filtered_data['tweet'].map(tweet_processor)\n",
    "filtered_data = filtered_data.rename(columns={\"tweet\": \"text\"})\n",
    "filtered_data = filtered_data.rename(columns={\"dialect_argmax\": \"t\"})\n",
    "filtered_data.to_json('../data/fdcl/train.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2177316/2834343217.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_data['tweet'] = filtered_data['tweet'].map(tweet_processor)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>t</th>\n",
       "      <th>ND_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t1864</td>\n",
       "      <td>&lt;user&gt; I am a MOD for # teamemmsie &lt;elongated&gt;...</td>\n",
       "      <td>hispanic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t23606</td>\n",
       "      <td>&lt;user&gt; We are sorry about this ! Please keep t...</td>\n",
       "      <td>white</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t45214</td>\n",
       "      <td>- come on babe that was just puppy love , it d...</td>\n",
       "      <td>white</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t35417</td>\n",
       "      <td>Delta buys pizza for passengers after cancelin...</td>\n",
       "      <td>white</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t52230</td>\n",
       "      <td>How crowdfunding helped a German college grad ...</td>\n",
       "      <td>white</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10965</th>\n",
       "      <td>t60228</td>\n",
       "      <td>\" \" \" Therefore , we are ambassadors for Chris...</td>\n",
       "      <td>white</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10966</th>\n",
       "      <td>t44207</td>\n",
       "      <td>Birds don ’ t just fly . They fall down and ge...</td>\n",
       "      <td>white</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10967</th>\n",
       "      <td>t76849</td>\n",
       "      <td>&lt;user&gt; and what have u done for syria ? nobody...</td>\n",
       "      <td>aav</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10968</th>\n",
       "      <td>t11162</td>\n",
       "      <td>&lt;user&gt; &lt;user&gt; But I am not from here &lt;happy&gt; b...</td>\n",
       "      <td>white</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10969</th>\n",
       "      <td>t47262</td>\n",
       "      <td>A shame &lt;user&gt; documentaries Dictatorland are ...</td>\n",
       "      <td>white</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10970 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                               text         t   \n",
       "0       t1864  <user> I am a MOD for # teamemmsie <elongated>...  hispanic  \\\n",
       "1      t23606  <user> We are sorry about this ! Please keep t...     white   \n",
       "2      t45214  - come on babe that was just puppy love , it d...     white   \n",
       "3      t35417  Delta buys pizza for passengers after cancelin...     white   \n",
       "4      t52230  How crowdfunding helped a German college grad ...     white   \n",
       "...       ...                                                ...       ...   \n",
       "10965  t60228  \" \" \" Therefore , we are ambassadors for Chris...     white   \n",
       "10966  t44207  Birds don ’ t just fly . They fall down and ge...     white   \n",
       "10967  t76849  <user> and what have u done for syria ? nobody...       aav   \n",
       "10968  t11162  <user> <user> But I am not from here <happy> b...     white   \n",
       "10969  t47262  A shame <user> documentaries Dictatorland are ...     white   \n",
       "\n",
       "       ND_label  \n",
       "0             0  \n",
       "1             0  \n",
       "2             0  \n",
       "3             0  \n",
       "4             0  \n",
       "...         ...  \n",
       "10965         0  \n",
       "10966         0  \n",
       "10967         1  \n",
       "10968         1  \n",
       "10969         0  \n",
       "\n",
       "[10970 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../data/fdcl/toxic_lang_data_pub/ND_founta_dev_dial_pAPI.csv')\n",
    "\n",
    "filtered_data = data[['id','tweet','dialect_argmax','ND_label']]\n",
    "filtered_data['tweet'] = filtered_data['tweet'].map(tweet_processor)\n",
    "filtered_data = filtered_data.rename(columns={\"tweet\": \"text\"})\n",
    "filtered_data = filtered_data.rename(columns={\"dialect_argmax\": \"t\"})\n",
    "filtered_data.to_json('../data/fdcl/validation.jsonl', orient='records', lines=True)\n",
    "\n",
    "filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2177316/4271741497.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_data['tweet'] = filtered_data['tweet'].map(tweet_processor)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>t</th>\n",
       "      <th>ND_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t60302</td>\n",
       "      <td>RT &lt;user&gt; : Jenna you are a fucking bitch &lt;url&gt;</td>\n",
       "      <td>white</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t78955</td>\n",
       "      <td>Desus and Mero discuss Trump ' s strange speec...</td>\n",
       "      <td>other</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t796</td>\n",
       "      <td>RT &lt;user&gt; : WE NEED TO FIND A FUCKING TREE HOU...</td>\n",
       "      <td>white</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t55661</td>\n",
       "      <td>Everybody wanna sell a Caesar salad with a nas...</td>\n",
       "      <td>aav</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t95922</td>\n",
       "      <td>Comedian Koffi Once Again Comes For Kiss Danie...</td>\n",
       "      <td>white</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12888</th>\n",
       "      <td>t66663</td>\n",
       "      <td>RT &lt;user&gt; : Mf ' s be having me fucked up ALL ...</td>\n",
       "      <td>aav</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12889</th>\n",
       "      <td>t1067</td>\n",
       "      <td>Day 1 of New York . Mostly exploring central p...</td>\n",
       "      <td>white</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12890</th>\n",
       "      <td>t9566</td>\n",
       "      <td>If you do not want to go through to website to...</td>\n",
       "      <td>white</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12891</th>\n",
       "      <td>t81228</td>\n",
       "      <td>Can dispatch release gong yoo and go eun datin...</td>\n",
       "      <td>white</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12892</th>\n",
       "      <td>t20681</td>\n",
       "      <td>&lt;user&gt; &lt;user&gt; &lt;user&gt; you are a straight hoe if...</td>\n",
       "      <td>white</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12893 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                               text      t   \n",
       "0      t60302    RT <user> : Jenna you are a fucking bitch <url>  white  \\\n",
       "1      t78955  Desus and Mero discuss Trump ' s strange speec...  other   \n",
       "2        t796  RT <user> : WE NEED TO FIND A FUCKING TREE HOU...  white   \n",
       "3      t55661  Everybody wanna sell a Caesar salad with a nas...    aav   \n",
       "4      t95922  Comedian Koffi Once Again Comes For Kiss Danie...  white   \n",
       "...       ...                                                ...    ...   \n",
       "12888  t66663  RT <user> : Mf ' s be having me fucked up ALL ...    aav   \n",
       "12889   t1067  Day 1 of New York . Mostly exploring central p...  white   \n",
       "12890   t9566  If you do not want to go through to website to...  white   \n",
       "12891  t81228  Can dispatch release gong yoo and go eun datin...  white   \n",
       "12892  t20681  <user> <user> <user> you are a straight hoe if...  white   \n",
       "\n",
       "       ND_label  \n",
       "0             1  \n",
       "1             0  \n",
       "2             1  \n",
       "3             0  \n",
       "4             0  \n",
       "...         ...  \n",
       "12888         1  \n",
       "12889         0  \n",
       "12890         0  \n",
       "12891         1  \n",
       "12892         1  \n",
       "\n",
       "[12893 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../data/fdcl/toxic_lang_data_pub/ND_founta_tst_dial_pAPI.csv')\n",
    "\n",
    "filtered_data = data[['id','tweet','dialect_argmax','ND_label']]\n",
    "filtered_data['tweet'] = filtered_data['tweet'].map(tweet_processor)\n",
    "filtered_data = filtered_data.rename(columns={\"tweet\": \"text\"})\n",
    "filtered_data = filtered_data.rename(columns={\"dialect_argmax\": \"t\"})\n",
    "filtered_data.to_json('../data/fdcl/test.jsonl', orient='records', lines=True)\n",
    "\n",
    "filtered_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "justisparse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
